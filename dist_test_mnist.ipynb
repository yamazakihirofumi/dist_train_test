{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb4eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e224172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/dourlin/miniconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/dourlin/miniconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'init_process' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from math import ceil\n",
    "from random import Random\n",
    "\n",
    "# ----- Model Definition -----\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# ----- Dataset Partitioning Classes -----\n",
    "class Partition(object):\n",
    "    def __init__(self, data, index):\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_idx = self.index[index]\n",
    "        return self.data[data_idx]\n",
    "\n",
    "class DataPartitioner(object):\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        data_len = len(data)\n",
    "        indexes = list(range(data_len))\n",
    "        rng.shuffle(indexes)\n",
    "\n",
    "        for frac in sizes:\n",
    "            part_len = int(frac * data_len)\n",
    "            self.partitions.append(indexes[:part_len])\n",
    "            indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "\n",
    "# ----- Helper Functions -----\n",
    "def get_test_loader():\n",
    "    test_dataset = datasets.MNIST(\n",
    "        './data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]))\n",
    "    return torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, filename):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, filename)\n",
    "\n",
    "# ----- Distributed Training Functions -----\n",
    "def partition_dataset():\n",
    "    dataset = datasets.MNIST(\n",
    "        './data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]))\n",
    "    \n",
    "    world_size = dist.get_world_size()\n",
    "    bsz = 128 // world_size\n",
    "    partition_sizes = [1.0 / world_size] * world_size\n",
    "    partition = DataPartitioner(dataset, partition_sizes)\n",
    "    partition = partition.use(dist.get_rank())\n",
    "    \n",
    "    train_set = torch.utils.data.DataLoader(\n",
    "        partition,\n",
    "        batch_size=bsz,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_set, bsz\n",
    "\n",
    "def average_gradients(model):\n",
    "    size = float(dist.get_world_size())\n",
    "    for param in model.parameters():\n",
    "        dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n",
    "        param.grad.data /= size\n",
    "\n",
    "# ----- Training Function -----\n",
    "def run(rank, size):\n",
    "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.manual_seed(1234 + rank)\n",
    "    \n",
    "    train_set, bsz = partition_dataset()\n",
    "    test_loader = get_test_loader() if rank == 0 else None\n",
    "    \n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for data, target in train_set:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            average_gradients(model)\n",
    "            optimizer.step()\n",
    "        \n",
    "        if rank == 0:\n",
    "            avg_loss = epoch_loss / len(train_set)\n",
    "            print(f'Epoch {epoch}, Loss: {avg_loss:.4f}')\n",
    "            \n",
    "            test_loss, accuracy = test(model, test_loader)\n",
    "            print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "            \n",
    "            save_checkpoint(model, optimizer, epoch, f'model_epoch_{epoch}.pth')\n",
    "\n",
    "# ----- Process Initialization -----\n",
    "def init_process(rank, size, fn, backend='gloo'):\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "def main():\n",
    "    world_size = 1\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "    \n",
    "    processes = []\n",
    "    for rank in range(world_size):\n",
    "        p = mp.Process(target=init_process, args=(rank, world_size, run))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a332e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ce201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
